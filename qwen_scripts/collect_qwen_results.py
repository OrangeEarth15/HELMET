#!/usr/bin/env python3
"""
æ”¶é›†Qwen XAT attentionæ–¹æ¡ˆçš„HELMETè¯„ä¼°ç»“æžœ
ä¸“é—¨ç”¨äºŽæ±‡æ€»Qwen2.5-7B-Instructçš„full, xattn, flex, xflexç­‰attentionæœºåˆ¶çš„ç»“æžœ
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import yaml
from dataclasses import dataclass, asdict
from tqdm import tqdm

# æ·»åŠ HELMETæ ¹ç›®å½•åˆ°è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
helmet_root = os.path.dirname(script_dir)
sys.path.insert(0, helmet_root)

# å¯¼å…¥collect_resultsçš„åŸºç¡€ç±»å’Œå‡½æ•°
import importlib.util
spec = importlib.util.spec_from_file_location('collect_results', os.path.join(helmet_root, 'scripts', 'collect_results.py'))
collect_results = importlib.util.module_from_spec(spec)
spec.loader.exec_module(collect_results)
arguments = collect_results.arguments
dataset_to_metrics = collect_results.dataset_to_metrics
custom_avgs = collect_results.custom_avgs

def main():
    """æ”¶é›†Qwen XAT attentionç»“æžœ"""
    
    # èŽ·å–HELMETæ ¹ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    helmet_root = os.path.dirname(script_dir)
    
    # ðŸŽ¯ Qwen XAT Attentioné…ç½® - æ ¹æ®å®žé™…å­˜åœ¨çš„ç›®å½•è°ƒæ•´
    qwen_configs = [
        # Full FlashInfer Attention
        {"model": "Qwen2.5-7B-Instruct", "tag": "qwen_full_flashattention", 
         "output_dir": os.path.join(helmet_root, "qwen_output", "full_flashattention"), "attention": "full"},
        
        # XAttention - ä¸åŒthreshold
        {"model": "Qwen2.5-7B-Instruct", "tag": "qwen_xattn_threshold0.95", 
         "output_dir": os.path.join(helmet_root, "qwen_output", "xattn_threshold0.95"), "attention": "xattn", "threshold": 0.95},
        
        # XAttention V6 - ä¸åŒthreshold
        {"model": "Qwen2.5-7B-Instruct", "tag": "qwen_xattn_v6_threshold0.95", 
         "output_dir": os.path.join(helmet_root, "qwen_output", "xattn_v6_threshold0.95"), "attention": "xattn_v6", "threshold": 0.95},
        
        # FlexPrefill - ä¸åŒgammaå’Œtau
        {"model": "Qwen2.5-7B-Instruct", "tag": "qwen_flex_gamma0.95_tau0.1", 
         "output_dir": os.path.join(helmet_root, "qwen_output", "flex_gamma0.95_tau0.1"), "attention": "flex", "gamma": 0.95, "tau": 0.1},
    ]

    # ðŸ“‹ æ•°æ®é›†é…ç½®æ–‡ä»¶
    config_files = [
        "configs/recall.yaml", "configs/recall_short.yaml", 
        "configs/rag.yaml", "configs/rag_short.yaml", 
        "configs/longqa.yaml", "configs/longqa_short.yaml", 
        "configs/summ.yaml", "configs/summ_short.yaml", 
        "configs/rerank.yaml", "configs/rerank_short.yaml", 
        "configs/icl.yaml", "configs/icl_short.yaml", 
        "configs/cite.yaml", "configs/cite_short.yaml", 
    ]

    # è§£æžæ•°æ®é›†é…ç½®
    dataset_configs = []
    
    for file in config_files:
        config_path = os.path.join(helmet_root, file)
        if not os.path.exists(config_path):
            print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            continue
            
        c = yaml.safe_load(open(config_path))
        
        if isinstance(c["generation_max_length"], int):
            c["generation_max_length"] = ",".join([str(c["generation_max_length"])] * len(c["datasets"].split(",")))
        for d, t, l, g in zip(c['datasets'].split(','), c['test_files'].split(','), c['input_max_length'].split(','), c['generation_max_length'].split(',')):
            # å¤„ç†ç©ºçš„test_files
            if t.strip() == '':
                test_name = ''
            else:
                test_name = os.path.basename(os.path.splitext(t)[0])
            
            # åŒ…å«æ‰€æœ‰é•¿åº¦é…ç½®
            input_len = int(l.strip())
            dataset_configs.append({
                "dataset": d.strip(), 
                "test_name": test_name, 
                "input_max_length": input_len, 
                "generation_max_length": int(g.strip()), 
                "max_test_samples": c['max_test_samples'], 
                'use_chat_template': c['use_chat_template'], 
                'shots': c['shots']
            })

    print(f"ðŸ“Š æ‰¾åˆ° {len(dataset_configs)} ä¸ªæ•°æ®é›†é…ç½®")
    print(f"ðŸŽ¯ å°†å¤„ç† {len(qwen_configs)} ä¸ªQwen XAT attentioné…ç½®")

    # æ”¶é›†ç»“æžœ
    failed_paths = []
    df = []
    
    for config in tqdm(qwen_configs, desc="æ”¶é›†Qwen XATç»“æžœ"):
        args = arguments()
        args.tag = config["tag"]
        args.output_dir = config["output_dir"]
        args.model = config["model"]
        
        print(f"\nðŸ“ å¤„ç†é…ç½®: {config['attention']} - {config['tag']}")
        print(f"   è¾“å‡ºç›®å½•: {args.output_dir}")
        
        # æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(args.output_dir):
            print(f"âš ï¸ è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {args.output_dir}")
            continue
        
        config_found_results = 0
        for dataset in dataset_configs:
            args.update(dataset)
            
            metric = args.get_averaged_metric()
            dsimple, mnames = args.get_metric_name()

            if metric is None:
                failed_paths.append(args.get_path())
                continue
                
            config_found_results += 1
            for k, m in metric.items():
                df.append({
                    **asdict(args), 
                    **config,
                    "metric name": k, 
                    "metric": m, 
                    "dataset_simple": dsimple + " " + k, 
                    "test_data": f"{args.dataset}-{args.test_name}-{args.input_max_length}"
                })
        
        print(f"   âœ… æ‰¾åˆ° {config_found_results} ä¸ªæœ‰æ•ˆç»“æžœ")

    if not df:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆç»“æžœï¼è¯·æ£€æŸ¥:")
        print("   1. è¾“å‡ºç›®å½•æ˜¯å¦å­˜åœ¨")
        print("   2. tagåç§°æ˜¯å¦æ­£ç¡®")
        print("   3. æ˜¯å¦æœ‰å®Œæˆçš„è¯„ä¼°ä»»åŠ¡")
        print("   4. æ˜¯å¦éœ€è¦è¿è¡ŒGPT-4è¯„ä¼°")
        return

    # ç”Ÿæˆæ±‡æ€»è¡¨æ ¼
    print(f"\nðŸ“ˆ ç”Ÿæˆæ±‡æ€»è¡¨æ ¼...")
    all_df = pd.DataFrame(df)
    
    # æŒ‰æ¨¡åž‹åˆ†ç»„ï¼Œä¸ºæ¯ä¸ªæ¨¡åž‹ç”Ÿæˆå•ç‹¬çš„CSVæ–‡ä»¶
    models = all_df['model'].unique()
    print(f"ðŸ“Š æ‰¾åˆ° {len(models)} ä¸ªæ¨¡åž‹: {list(models)}")
    
    for model in models:
        print(f"\nðŸ”„ å¤„ç†æ¨¡åž‹: {model}")
        model_df = all_df[all_df['model'] == model].copy()
        
        # åˆ›å»ºé€è§†è¡¨
        lf_df = model_df.pivot_table(
            index=["input_max_length", "attention", "tag"], 
            columns="dataset_simple", 
            values="metric", 
            sort=False
        )
        lf_df = lf_df.reset_index()

        # è®¡ç®—è‡ªå®šä¹‰å¹³å‡å€¼
        for k, v in custom_avgs.items():
            available_cols = [col for col in v if col in lf_df.columns]
            if available_cols:
                lf_df[k] = lf_df[available_cols].mean(axis=1)
            else:
                print(f"âš ï¸ è·³è¿‡ {k}: ç¼ºå°‘å¿…è¦çš„åˆ—")

        # ä¿å­˜ç»“æžœ - ä¸ºæ¯ä¸ªæ¨¡åž‹ç”Ÿæˆå•ç‹¬çš„CSVæ–‡ä»¶
        model_name = model.replace("Qwen", "qwen").replace("-", "_").replace(".", "").lower()
        output_file = os.path.join(helmet_root, f"{model_name}_results_summary.csv")
        lf_df.to_csv(output_file, index=False)
        
        print(f"âœ… {model} ç»“æžœå·²ä¿å­˜åˆ°: {output_file}")
        print(f"ðŸ“Š å…±å¤„ç†äº† {len(model_df)} ä¸ªæ•°æ®ç‚¹")
        
        # æ˜¾ç¤ºé¢„è§ˆ
        print(f"\nðŸ“‹ {model} ç»“æžœé¢„è§ˆ:")
        available_custom_cols = [col for col in custom_avgs.keys() if col in lf_df.columns]
        if available_custom_cols:
            print(lf_df[['input_max_length', 'attention', 'tag'] + available_custom_cols].to_string(index=False))

    if failed_paths:
        print(f"\nâš ï¸ ä»¥ä¸‹ {len(failed_paths)} ä¸ªè·¯å¾„çš„ç»“æžœæœªæ‰¾åˆ°:")
        for path in failed_paths[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"   {path}")
        if len(failed_paths) > 10:
            print(f"   ... è¿˜æœ‰ {len(failed_paths)-10} ä¸ª")

    return lf_df, failed_paths

if __name__ == "__main__":
    main()
