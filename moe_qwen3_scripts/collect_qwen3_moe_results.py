#!/usr/bin/env python3
"""
HELMET Qwen3 MoE ç»“æœæ”¶é›†å’Œåˆ†æè„šæœ¬

åŠŸèƒ½:
1. æ”¶é›†æ‰€æœ‰Qwen3 MoEå®éªŒç»“æœ
2. ç”Ÿæˆç»Ÿä¸€çš„CSVæŠ¥å‘Š
3. å¯¹æ¯”ä¸åŒattentionæ–¹æ³•çš„æ€§èƒ½
4. ç”ŸæˆMoEç‰¹æœ‰çš„åˆ†ææŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
    python moe_qwen3_scripts/collect_qwen3_moe_results.py [--output output.csv]
"""

import os
import json
import pandas as pd
import argparse
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def find_result_files(base_dir: str = "moe_qwen3_output") -> List[str]:
    """é€’å½’æŸ¥æ‰¾æ‰€æœ‰ç»“æœæ–‡ä»¶"""
    result_files = []
    base_path = Path(base_dir)
    
    if not base_path.exists():
        logger.warning(f"è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {base_dir}")
        return result_files
    
    # æŸ¥æ‰¾æ‰€æœ‰.jsonæ–‡ä»¶ï¼ˆä½†ä¸åŒ…æ‹¬.scoreæ–‡ä»¶ï¼‰
    for json_file in base_path.rglob("*.json"):
        if not str(json_file).endswith(".score"):
            result_files.append(str(json_file))
    
    logger.info(f"å‘ç° {len(result_files)} ä¸ªç»“æœæ–‡ä»¶")
    return result_files

def parse_filename(filepath: str) -> Dict[str, str]:
    """ä»æ–‡ä»¶è·¯å¾„è§£æå®éªŒå‚æ•°"""
    parts = filepath.split("/")
    filename = parts[-1]
    
    info = {
        "filepath": filepath,
        "method_dir": parts[-3] if len(parts) >= 3 else "unknown",
        "task_dir": parts[-2] if len(parts) >= 2 else "unknown", 
        "filename": filename
    }
    
    # ä»ç›®å½•åè§£æattentionæ–¹æ³•å’Œå‚æ•°
    method_dir = info["method_dir"]
    if "full" in method_dir:
        info["attention_method"] = "full"
        info["threshold"] = None
        info["score_ratio"] = None
        info["gamma"] = None
        info["tau"] = None
        info["stride"] = None
    elif "xattn" in method_dir:
        info["attention_method"] = "xattn"
        # è§£æthreshold
        if "threshold" in method_dir:
            parts = method_dir.split("threshold")
            if len(parts) > 1:
                info["threshold"] = float(parts[1].replace("_", "."))
        info["stride"] = 16  # é»˜è®¤å€¼
    elif "xflex" in method_dir:
        info["attention_method"] = "xflex"
        # è§£æthresholdå’Œscore_ratio
        if "threshold" in method_dir and "scoreratio" in method_dir:
            parts = method_dir.split("_")
            for part in parts:
                if part.startswith("threshold"):
                    info["threshold"] = float(part.replace("threshold", ""))
                elif part.startswith("scoreratio"):
                    info["score_ratio"] = float(part.replace("scoreratio", ""))
        info["stride"] = 16  # é»˜è®¤å€¼
    elif "flex" in method_dir:
        info["attention_method"] = "flex"
        # è§£ægammaå’Œtau
        if "gamma" in method_dir and "tau" in method_dir:
            parts = method_dir.split("_")
            for part in parts:
                if part.startswith("gamma"):
                    info["gamma"] = float(part.replace("gamma", ""))
                elif part.startswith("tau"):
                    info["tau"] = float(part.replace("tau", ""))
    
    # ä»æ–‡ä»¶åè§£æä»»åŠ¡å’Œå…¶ä»–ä¿¡æ¯
    if "_" in filename:
        name_parts = filename.replace(".json", "").split("_")
        for i, part in enumerate(name_parts):
            if part in ["recall", "rag", "longqa", "summ", "icl", "rerank", "cite"]:
                info["task"] = part
                break
        
        # è§£æè¾“å…¥é•¿åº¦
        for part in name_parts:
            if part.startswith("in") and part[2:].isdigit():
                info["input_length"] = int(part[2:])
                break
    
    return info

def load_result_data(filepath: str) -> Dict:
    """åŠ è½½å•ä¸ªç»“æœæ–‡ä»¶çš„æ•°æ®"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except Exception as e:
        logger.error(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
        return {}

def extract_metrics(data: Dict, file_info: Dict) -> Dict:
    """ä»ç»“æœæ•°æ®ä¸­æå–å…³é”®æŒ‡æ ‡"""
    metrics = {
        "model": "Qwen3-30B-A3B-Instruct-MoE",
        "attention_method": file_info.get("attention_method", "unknown"),
        "task": file_info.get("task", "unknown"),
        "input_length": file_info.get("input_length", 0),
        "threshold": file_info.get("threshold"),
        "score_ratio": file_info.get("score_ratio"),
        "gamma": file_info.get("gamma"),
        "tau": file_info.get("tau"),
        "stride": file_info.get("stride"),
        "filepath": file_info["filepath"]
    }
    
    # æå–å¹³å‡æŒ‡æ ‡
    if "averaged_metrics" in data:
        avg_metrics = data["averaged_metrics"]
        for key, value in avg_metrics.items():
            if isinstance(value, (int, float)):
                metrics[f"avg_{key}"] = value
    
    # æå–æ€§èƒ½æŒ‡æ ‡
    if "throughput" in data:
        metrics["throughput"] = data["throughput"]
    
    if "memory_usage" in data:
        metrics["memory_usage_gb"] = data["memory_usage"] / (1024**3)
    
    # è®¡ç®—ä¸€äº›æ´¾ç”ŸæŒ‡æ ‡
    if "avg_input_len" in metrics and "avg_output_len" in metrics:
        metrics["total_tokens"] = metrics["avg_input_len"] + metrics["avg_output_len"]
    
    return metrics

def generate_summary_report(df: pd.DataFrame) -> str:
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    report = []
    report.append("=" * 80)
    report.append("HELMET Qwen3-30B-A3B-Instruct MoE è¯„ä¼°ç»“æœæ±‡æ€»")
    report.append("=" * 80)
    report.append("")
    
    # åŸºæœ¬ç»Ÿè®¡
    report.append("ğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
    report.append(f"  â€¢ æ€»å®éªŒæ•°: {len(df)}")
    report.append(f"  â€¢ æ³¨æ„åŠ›æ–¹æ³•: {', '.join(df['attention_method'].unique())}")
    report.append(f"  â€¢ è¯„ä¼°ä»»åŠ¡: {', '.join(df['task'].unique())}")
    report.append(f"  â€¢ è¾“å…¥é•¿åº¦èŒƒå›´: {df['input_length'].min()}-{df['input_length'].max()}")
    report.append("")
    
    # æŒ‰attentionæ–¹æ³•åˆ†ç»„çš„æ€§èƒ½å¯¹æ¯”
    if 'avg_exact_match' in df.columns:
        report.append("ğŸ¯ å„æ³¨æ„åŠ›æ–¹æ³•æ€§èƒ½å¯¹æ¯” (Exact Match):")
        method_performance = df.groupby('attention_method')['avg_exact_match'].agg(['mean', 'std', 'count'])
        for method, row in method_performance.iterrows():
            report.append(f"  â€¢ {method:10}: {row['mean']:6.2f}% Â± {row['std']:5.2f}% (n={row['count']})")
        report.append("")
    
    # æŒ‰ä»»åŠ¡åˆ†ç»„çš„æ€§èƒ½
    if 'avg_exact_match' in df.columns:
        report.append("ğŸ“‹ å„ä»»åŠ¡æ€§èƒ½å¯¹æ¯” (Exact Match):")
        task_performance = df.groupby('task')['avg_exact_match'].agg(['mean', 'std', 'count'])
        for task, row in task_performance.iterrows():
            report.append(f"  â€¢ {task:10}: {row['mean']:6.2f}% Â± {row['std']:5.2f}% (n={row['count']})")
        report.append("")
    
    # æ€§èƒ½ vs æ•ˆç‡åˆ†æ
    if 'throughput' in df.columns and 'avg_exact_match' in df.columns:
        report.append("âš¡ æ€§èƒ½ä¸æ•ˆç‡åˆ†æ:")
        for method in df['attention_method'].unique():
            method_data = df[df['attention_method'] == method]
            if len(method_data) > 0:
                avg_acc = method_data['avg_exact_match'].mean()
                avg_throughput = method_data['throughput'].mean() if 'throughput' in method_data.columns else 0
                report.append(f"  â€¢ {method:10}: å‡†ç¡®ç‡={avg_acc:5.2f}%, ååé‡={avg_throughput:5.2f} samples/s")
        report.append("")
    
    # MoEç‰¹æœ‰åˆ†æ
    report.append("ğŸ”§ MoEæ¶æ„ç‰¹æ€§åˆ†æ:")
    report.append("  â€¢ ä¸“å®¶æ•°é‡: 128ä¸ªä¸“å®¶")
    report.append("  â€¢ æ¿€æ´»ä¸“å®¶: æ¯tokenæ¿€æ´»8ä¸ªä¸“å®¶")
    report.append("  â€¢ ä¸“é—¨ä¼˜åŒ–: ä½¿ç”¨load_qwen3_moe.pyå®ç°")
    if 'memory_usage_gb' in df.columns:
        avg_memory = df['memory_usage_gb'].mean()
        report.append(f"  â€¢ å¹³å‡å†…å­˜ä½¿ç”¨: {avg_memory:.1f}GB")
    report.append("")
    
    # æœ€ä½³é…ç½®æ¨è
    if 'avg_exact_match' in df.columns:
        report.append("ğŸ† æ¨èé…ç½®:")
        best_overall = df.loc[df['avg_exact_match'].idxmax()]
        report.append(f"  â€¢ æœ€ä½³æ•´ä½“æ€§èƒ½: {best_overall['attention_method']} "
                     f"(å‡†ç¡®ç‡: {best_overall['avg_exact_match']:.2f}%)")
        
        if 'throughput' in df.columns:
            best_efficiency = df.loc[df['throughput'].idxmax()]
            report.append(f"  â€¢ æœ€ä½³æ•ˆç‡: {best_efficiency['attention_method']} "
                         f"(ååé‡: {best_efficiency['throughput']:.2f} samples/s)")
        report.append("")
    
    return "\n".join(report)

def main():
    parser = argparse.ArgumentParser(description="æ”¶é›†å’Œåˆ†æQwen3 MoEå®éªŒç»“æœ")
    parser.add_argument("--output", "-o", default="moe_qwen3_results_summary.csv", 
                       help="è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--base_dir", default="moe_qwen3_output",
                       help="ç»“æœæ–‡ä»¶åŸºç¡€ç›®å½•")
    parser.add_argument("--report", default="moe_qwen3_results_report.txt",
                       help="æ±‡æ€»æŠ¥å‘Šæ–‡ä»¶è·¯å¾„") 
    args = parser.parse_args()
    
    logger.info("å¼€å§‹æ”¶é›†Qwen3 MoEå®éªŒç»“æœ...")
    
    # æŸ¥æ‰¾æ‰€æœ‰ç»“æœæ–‡ä»¶
    result_files = find_result_files(args.base_dir)
    
    if not result_files:
        logger.error("æœªæ‰¾åˆ°ä»»ä½•ç»“æœæ–‡ä»¶")
        return
    
    # å¤„ç†æ¯ä¸ªç»“æœæ–‡ä»¶
    all_metrics = []
    for filepath in result_files:
        logger.info(f"å¤„ç†æ–‡ä»¶: {filepath}")
        
        file_info = parse_filename(filepath)
        data = load_result_data(filepath)
        
        if data:
            metrics = extract_metrics(data, file_info)
            all_metrics.append(metrics)
    
    if not all_metrics:
        logger.error("æœªèƒ½æå–ä»»ä½•æœ‰æ•ˆæ•°æ®")
        return
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(all_metrics)
    
    # ä¿å­˜CSV
    df.to_csv(args.output, index=False)
    logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
    
    # ç”Ÿæˆå¹¶ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    report = generate_summary_report(df)
    with open(args.report, 'w', encoding='utf-8') as f:
        f.write(report)
    logger.info(f"æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.report}")
    
    # æ‰“å°åŸºæœ¬ç»Ÿè®¡
    print("\n" + "="*50)
    print("Qwen3 MoEç»“æœæ”¶é›†å®Œæˆ!")
    print("="*50)
    print(f"ğŸ“Š å¤„ç†æ–‡ä»¶æ•°: {len(result_files)}")
    print(f"ğŸ“ˆ æœ‰æ•ˆå®éªŒæ•°: {len(all_metrics)}")
    print(f"ğŸ’¾ CSVè¾“å‡º: {args.output}")
    print(f"ğŸ“„ æŠ¥å‘Šè¾“å‡º: {args.report}")
    
    if len(df) > 0:
        print(f"ğŸ¯ æ³¨æ„åŠ›æ–¹æ³•: {', '.join(df['attention_method'].unique())}")
        print(f"ğŸ“‹ è¯„ä¼°ä»»åŠ¡: {', '.join(df['task'].unique())}")
        
        if 'avg_exact_match' in df.columns:
            best_method = df.loc[df['avg_exact_match'].idxmax(), 'attention_method']
            best_score = df['avg_exact_match'].max()
            print(f"ğŸ† æœ€ä½³æ–¹æ³•: {best_method} ({best_score:.2f}%)")

if __name__ == "__main__":
    main()
