#!/bin/bash

# HELMET Qwen3-30B-A3B-Instruct MoE Full Attention è¯„ä¼°è„šæœ¬
echo "Running HELMET with Qwen3-30B-A3B-Instruct MoE Full Attention"

# åˆ‡æ¢åˆ°HELMETæ ¹ç›®å½•
cd "$(dirname "$0")/.."

# ğŸ¯ è®¾ç½®ç¼“å­˜å’Œç¯å¢ƒå˜é‡
export HF_HOME="/home/scratch.sarawang_ent/project/HELMET/.hf_cache"
export TRANSFORMERS_CACHE="/home/scratch.sarawang_ent/project/HELMET/.hf_cache/transformers"
export HF_DATASETS_CACHE="/home/scratch.sarawang_ent/project/HELMET/.hf_cache/datasets"
export HF_HUB_CACHE="/home/scratch.sarawang_ent/project/HELMET/.hf_cache/hub"
export TORCH_HOME="/home/scratch.sarawang_ent/project/HELMET/.torch_cache"
export MODELSCOPE_CACHE="/home/scratch.sarawang_ent/modelscope_cache"
export USE_MODELSCOPE_HUB=1

# ğŸš¨ MoEæ¨¡å‹ç‰¹æ®Šé…ç½®
export CUDA_VISIBLE_DEVICES=0,1,2,3  # å¤šGPUæ”¯æŒMoE
export OMP_NUM_THREADS=4
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# åˆ›å»ºç¼“å­˜ç›®å½•
mkdir -p "$HF_HOME"
mkdir -p "$TRANSFORMERS_CACHE"
mkdir -p "$HF_DATASETS_CACHE"
mkdir -p "$HF_HUB_CACHE"
mkdir -p "$TORCH_HOME"
mkdir -p "$MODELSCOPE_CACHE"

# è®¾ç½®Qwen3-30B-A3B-Instructæ¨¡å‹è·¯å¾„
MODEL_NAME=${1:-"/home/scratch.sarawang_ent/modelscope_cache/Qwen/Qwen3-30B-A3B-Instruct-2507"}

# è®¾ç½®è¾“å‡ºç›®å½•
export OUTPUT_DIR="moe_qwen3_output/full_attention"
mkdir -p $OUTPUT_DIR

echo "ğŸ”§ GPU Memory check: $(nvidia-smi --query-gpu=memory.total,memory.used --format=csv,noheader,nounits | head -1)"
echo "ğŸ“ Model: Qwen3-30B-A3B-Instruct MoE (128ä¸“å®¶, 8ä¸“å®¶/token)"  
echo "ğŸ¯ Attention: Full Attention (baseline)"

echo "Running 8k-64k versions with Qwen3-30B-A3B-Instruct MoE Full Attention"
for task in "recall" "rag" "longqa" "summ" "icl" "rerank" "cite"; do
    echo "Running task: $task (8k-64k) with Qwen3 MoE full attention"
    mkdir -p $OUTPUT_DIR/$task
    python eval.py \
        --config configs/${task}_short.yaml \
        --model_name_or_path $MODEL_NAME \
        --attn_metric full \
        --tag qwen3_moe_full_attention \
        --output_dir $OUTPUT_DIR/$task \
        --max_test_samples 50 \
        --num_workers 2
done

echo "Running 128k versions with Qwen3-30B-A3B-Instruct MoE Full Attention"
for task in "recall" "rag" "longqa" "summ" "icl" "rerank" "cite"; do
    echo "Running task: $task (128k) with Qwen3 MoE full attention"
    mkdir -p $OUTPUT_DIR/$task
    python eval.py \
        --config configs/${task}.yaml \
        --model_name_or_path $MODEL_NAME \
        --attn_metric full \
        --tag qwen3_moe_full_attention \
        --output_dir $OUTPUT_DIR/$task \
        --max_test_samples 50 \
        --num_workers 2
done

echo "ğŸ‰ Qwen3-30B MoE Full Attention evaluation completed! Results in $OUTPUT_DIR"
echo "ğŸ“Š MoEæ¶æ„: 128ä¸“å®¶, 8ä¸“å®¶/tokenæ¿€æ´»"
echo "ğŸ’¡ æ­¤ä¸ºåŸºå‡†æµ‹è¯•ï¼Œç”¨äºå¯¹æ¯”ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•çš„æ€§èƒ½"