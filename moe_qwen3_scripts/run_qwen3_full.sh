#!/bin/bash

# HELMET Qwen3-30B-A3B-Instruct MoE FlashInfer Full Attention è¯„ä¼°è„šæœ¬
echo "Running HELMET with Qwen3-30B-A3B-Instruct MoE Full Attention"

# åˆ‡æ¢åˆ°HELMETæ ¹ç›®å½•
cd "$(dirname "$0")/.."

# ğŸ¯ è®¾ç½®è‡ªå®šä¹‰ç¼“å­˜è·¯å¾„åˆ°é¡¹ç›®ç›®å½•ä¸‹ï¼ˆé¿å…å ç”¨homeç©ºé—´ï¼‰
export HF_HOME="/home/scratch.sarawang_ent/project/HELMET/.hf_cache"
export TRANSFORMERS_CACHE="/home/scratch.sarawang_ent/project/HELMET/.hf_cache/transformers"
export HF_DATASETS_CACHE="/home/scratch.sarawang_ent/project/HELMET/.hf_cache/datasets"
export HF_HUB_CACHE="/home/scratch.sarawang_ent/project/HELMET/.hf_cache/hub"
export TORCH_HOME="/home/scratch.sarawang_ent/project/HELMET/.torch_cache"

# ğŸ¯ è®¾ç½®ModelScopeé­”å¡”é•œåƒ
export MODELSCOPE_CACHE="/home/scratch.sarawang_ent/modelscope_cache"
export USE_MODELSCOPE_HUB=1

# ğŸš¨ MoEæ¨¡å‹ç‰¹æ®Šé…ç½®
export CUDA_VISIBLE_DEVICES=0  # æ ¹æ®GPUé…ç½®è°ƒæ•´
export OMP_NUM_THREADS=4       # æ§åˆ¶CPUçº¿ç¨‹æ•°
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512  # å†…å­˜åˆ†ç‰‡ä¼˜åŒ–

echo "ğŸ’¡ é‡å®šå‘æ‰€æœ‰ç¼“å­˜åˆ°é¡¹ç›®ç›®å½•ï¼Œé¿å…å¡«æ»¡ ~/.cache"

# åˆ›å»ºç¼“å­˜ç›®å½•
mkdir -p "$HF_HOME"
mkdir -p "$TRANSFORMERS_CACHE"
mkdir -p "$HF_DATASETS_CACHE"
mkdir -p "$HF_HUB_CACHE"
mkdir -p "$TORCH_HOME"
mkdir -p "$MODELSCOPE_CACHE"

echo "ğŸ—‚ï¸ Cache directories set to:"
echo "  HF_HOME: $HF_HOME"
echo "  HF_DATASETS_CACHE: $HF_DATASETS_CACHE"
echo "  HF_HUB_CACHE: $HF_HUB_CACHE"
echo "  TORCH_HOME: $TORCH_HOME"
echo "  MODELSCOPE_CACHE: $MODELSCOPE_CACHE"

# è®¾ç½®Qwen3-30B-A3B-Instructæ¨¡å‹è·¯å¾„
MODEL_NAME=${1:-"/home/scratch.sarawang_ent/modelscope_cache/Qwen/Qwen3-30B-A3B-Instruct-2507"}

# è®¾ç½®è¾“å‡ºç›®å½•
export OUTPUT_DIR="moe_qwen3_output/full_flashinfer"
mkdir -p $OUTPUT_DIR

echo "ğŸ”§ GPU Memory check: $(nvidia-smi --query-gpu=memory.total,memory.used --format=csv,noheader,nounits | head -1)"
echo "ğŸ“ Model: Qwen3-30B-A3B-Instruct MoE"
echo "ğŸ¯ Attention: Full FlashInfer"

echo "Running 8k-64k versions with Qwen3-30B-A3B-Instruct MoE Full Attention"
for task in "recall" "rag" "longqa" "summ" "icl" "rerank" "cite"; do
    echo "Running task: $task (8k-64k) with Qwen3 MoE full attention"
    python eval.py \
        --config configs/${task}_short.yaml \
        --model_name_or_path $MODEL_NAME \
        --attn_metric full \
        --tag qwen3_moe_full_flashinfer \
        --output_dir $OUTPUT_DIR/$task \
        --max_test_samples 50 \
        --num_workers 2
done

echo "Running 128k versions with Qwen3-30B-A3B-Instruct MoE Full Attention"
for task in "recall" "rag" "longqa" "summ" "icl" "rerank" "cite"; do
    echo "Running task: $task with Qwen3 MoE full attention"
    python eval.py \
        --config configs/${task}.yaml \
        --model_name_or_path $MODEL_NAME \
        --attn_metric full \
        --tag qwen3_moe_full_flashinfer \
        --output_dir $OUTPUT_DIR/$task \
        --max_test_samples 50 \
        --num_workers 2
done

echo "Qwen3-30B MoE Full attention evaluation completed! Results in $OUTPUT_DIR"
